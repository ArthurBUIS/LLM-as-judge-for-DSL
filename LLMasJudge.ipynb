{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from apikey import api_key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a table Catalog containing different items and their color. \"red\" should be a color cited. Create another table containing the same dimension but only with the red items. Show the Labels of the red items. ```envision\n",
      "table Catalog = with \n",
      "  [| as Label, as Color |]\n",
      "  [| \"Socks\" , \"red\"    |]\n",
      "  [| \"Socks\" , \"blue\"   |]\n",
      "  [| \"Shirt\" , \"red\"    |] \n",
      " \n",
      "table smallCatalog = where Catalog.Color==\"red\" // broadcast during definition\n",
      " \n",
      "show table \"red items\" with\n",
      "  smallCatalog.Label\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def readfile(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read()\n",
    "docu =readfile(\"envision-brief.md\")    \n",
    "challenge = readfile(\"mychallenges/c\"+\"008\"+\".md\")\n",
    "\n",
    "# decompose challenge into question and prof_answer\n",
    "def decompose_challenge(challenge):\n",
    "    question,prof_answer=challenge.split(\"\\n\\n# ANSWER\\n\\n\")\n",
    "    return question, prof_answer\n",
    "question,prof_answer=decompose_challenge(challenge)\n",
    "print(question,prof_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```envision\n",
      "table Catalog = with\n",
      "  [| as Item, as Color |]\n",
      "  [| \"Apple\", \"red\" |]\n",
      "  [| \"Banana\", \"yellow\" |]\n",
      "  [| \"Cherry\", \"red\" |]\n",
      "  [| \"Grapes\", \"green\" |]\n",
      "\n",
      "table RedItems = where Catalog.Color == \"red\"\n",
      "\n",
      "show table \"Red Items\" a1b3 with\n",
      "  RedItems.Item\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "coder_personality=\"You are a proficient coder in the Domain Specific Language called Envision. \\\n",
    "    Your task is to generate response to the given challenge. \\\n",
    "    Some challenges will ask you to generate Envision code,\\\n",
    "    others will ask you to explain given code or answer questions related to the Envision language. \\\n",
    "    Do not output any intermediate thinking, only give the final answer.\\\n",
    "    Here is the documentation of Envision:\\\n",
    "    ### Documentation\\n\" + docu\n",
    "coder_prompt=question\n",
    "\n",
    "coder_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": coder_personality},\n",
    "        {\"role\": \"user\", \"content\": coder_prompt}\n",
    "    ],\n",
    "    max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.2,\n",
    ")\n",
    "stud_sentence=coder_response.choices[0].message.content\n",
    "print(stud_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### QUESTION: Create a table Catalog containing different items and their color. \"red\" should be a color cited. Create another table containing the same dimension but only with the red items. Show the Labels of the red items.\n",
      "### PROFESSOR ANSWER: ```envision\n",
      "table Catalog = with \n",
      "  [| as Label, as Color |]\n",
      "  [| \"Socks\" , \"red\"    |]\n",
      "  [| \"Socks\" , \"blue\"   |]\n",
      "  [| \"Shirt\" , \"red\"    |] \n",
      " \n",
      "table smallCatalog = where Catalog.Color==\"red\" // broadcast during definition\n",
      " \n",
      "show table \"red items\" with\n",
      "  smallCatalog.Label\n",
      "```\n",
      "### STUDENT ANSWER: ```envision\n",
      "table Catalog = with\n",
      "  [| as Item, as Color |]\n",
      "  [| \"Apple\", \"red\" |]\n",
      "  [| \"Banana\", \"yellow\" |]\n",
      "  [| \"Cherry\", \"red\" |]\n",
      "  [| \"Grapes\", \"green\" |]\n",
      "\n",
      "table RedItems = where Catalog.Color == \"red\"\n",
      "\n",
      "show table \"Red Items\" a1b3 with\n",
      "  RedItems.Item\n",
      "```\n",
      "- The STUDENT ANSWER creates a table named Catalog with columns named Item and Color, which is different from the columns named Label and Color in the PROFESSOR ANSWER. This difference is ACCEPTABLE.\n",
      "- The STUDENT ANSWER includes items like \"Apple\", \"Banana\", \"Cherry\", and \"Grapes\" with their respective colors, which is different from the items in the PROFESSOR ANSWER. This difference is ACCEPTABLE.\n",
      "- The STUDENT ANSWER creates a table named RedItems using the condition Catalog.Color == \"red\", which is the same logic as in the PROFESSOR ANSWER. This similarity is ACCEPTABLE.\n",
      "- The STUDENT ANSWER shows the table with the label \"Red Items\" and displays the Item column from the RedItems table. The position label a1b3 is added to the show command, which is ACCEPTABLE.\n",
      "\n",
      "Therefore, the STUDENT ANSWER is OK.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this personality sticks more to professor's answer.\n",
    "\n",
    "judge_personality_teacherAuthority=\"Your goal is to judge whether the STUDENT ANSWER is OK or NOT_OK, as an answer to the QUESTION.\\\n",
    "In order to judge the STUDENT ANSWER, you are given the PROFESSOR ANSWER which is always OK.\\\n",
    "You should judge NOT_OK if there is something UNACCEPTABLE in the STUDENT ANSWER; and judge OK if everything is ACCEPTABLE.\\\n",
    "Your main job is not to check the syntax correctness, but the logical correctness.\\\n",
    "If the STUDENT ANSWER does not treat the QUESTION logically, it is UNACCEPTABLE.\\\n",
    "Pay special attention to the comments in the PROFESSOR ANSWER. If these comments include\\\n",
    "a rule and if the STUDENT ANSWER violates it, this is UNACCEPTABLE.\\\n",
    "Adding a print position label like a1b2 in the show command is ACCEPTABLE, as long as this is not forbidden by\\\n",
    "the QUESTION or the comments in the PROFESSOR ANSWER.\\\n",
    "Differences in variable names, column names and table names shall systematically be ACCEPTABLE, \\\n",
    "as long as it is not specifically required in the QUESTION or in the comments of the PROFESSOR ANSWER.\\\n",
    "There are sometimes various ways or logics to treat the same QUESTION, and this is ACCEPTABLE, as long as the goal of the QUESTION is achieved.\\\n",
    "Let's think aloud step by step to help your judgement. Tell each ACCEPTABLE or UNACCEPTABLE point. \\\n",
    "At the end of your output, you must add a seperate line which ONLY contains your judgement as expressed in '0' for NOT_OK or '1' for OK,\\\n",
    "according to your previous conclusions, and NOTHING MORE!\"\n",
    "\n",
    "judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "\n",
    "# Generate a response from the chatbot\n",
    "judge_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": judge_personality_teacherAuthority},\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ],\n",
    "    max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.2,\n",
    ")\n",
    "print(judge_prompt)\n",
    "# Print the generated response\n",
    "print(judge_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table Catalog = with\n",
      "  [| as Item, as Color |]\n",
      "  [| \"Apple\", \"red\" |]\n",
      "  [| \"Banana\", \"yellow\" |]\n",
      "  [| \"Cherry\", \"red\" |]\n",
      "  [| \"Grapes\", \"green\" |]\n",
      "\n",
      "table RedItems = where Catalog.Color == \"red\"\n",
      "\n",
      "show table \"Red Items\" a1b3 with\n",
      "  RedItems.Item\n"
     ]
    }
   ],
   "source": [
    "# extract the 'real' code from the student answer (cut away the '''envision bit at the start and end)\n",
    "def extract_code(stud_sentence):\n",
    "    lines = stud_sentence.strip().split('\\n')\n",
    "    return '\\n'.join(lines[1:-1])\n",
    "print(extract_code(stud_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "# send code to online compiler and check if it compiles\n",
    "\n",
    "def check_compilation(script):\n",
    "    url = \"https://try.lokad.com/w/script/trycompile\"\n",
    "    payload = {\n",
    "        \"Script\": script\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send POST request\n",
    "        response = requests.post(url, json=payload)\n",
    "\n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if result[\"IsCompOk\"]:\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Compilation Failed!\")\n",
    "                for message in result[\"CompMessages\"]:\n",
    "                    print(f\"Error: {message['Text']} (Line: {message['Line']}, Start: {message['Start']}, Length: {message['Length']}, Severity: {message['Severity']})\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(\"Error: Unable to reach the compilation service.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "check_compilation(extract_code(stud_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each question, try 3 generation-compilations.\n",
    "# if compiles, further check with judge.\n",
    "def pipeline_verify(challenge,coder_personality,judge_personality=judge_personality_teacherAuthority):\n",
    "    question,prof_answer=decompose_challenge(challenge)\n",
    "    n_tries=3\n",
    "    for compile_try in range(n_tries):\n",
    "        coder_prompt=question\n",
    "        coder_response = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": coder_personality},\n",
    "                {\"role\": \"user\", \"content\": coder_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        stud_sentence=coder_response.choices[0].message.content\n",
    "        if(check_compilation(extract_code(stud_sentence))):\n",
    "            print('# compile ok')\n",
    "            break\n",
    "        elif (compile_try==n_tries-1):\n",
    "            print( \"# too many failures !\")\n",
    "            print('# badcode:\\n'+extract_code(stud_sentence))\n",
    "            return stud_sentence,\"too many failures !\",False\n",
    "\n",
    "    judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "    judge_response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge_personality},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ],\n",
    "        max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    judge_sentence=judge_response.choices[0].message.content\n",
    "    judge_decision=judge_sentence[-1]=='1'\n",
    "    print ('# judge_decision:',judge_decision)\n",
    "    if not judge_decision:\n",
    "        print('# badcode:\\n',extract_code(stud_sentence))\n",
    "        print('# judge explanation:\\n',judge_sentence)\n",
    "    return stud_sentence,judge_sentence,judge_decision\n",
    "# a all-in-one function to score a model on a list of challenges\n",
    "def pipeline_score_allchallenge(indexes,coder_personality):\n",
    "    challenges=[readfile(\"mychallenges/c\"+index+\".md\") for index in indexes]\n",
    "    score=0\n",
    "    for i in range(len(challenges)):\n",
    "        challenge=challenges[i]\n",
    "        print('\\n### verifying challenge No. '+indexes[i])\n",
    "        _,_,judge_decision=pipeline_verify(challenge,coder_personality)\n",
    "        if (judge_decision): score+=1\n",
    "    print('correct:'+str(score)+' out of '+str(len(challenges))+', '+str(score/len(challenges)*100)+'%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### verifying challenge No. 000\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 5, Start: 20, Length: 4, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 5, Start: 20, Length: 4, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 2, Start: 20, Length: 4, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "greetings = \"Hello\"\n",
      "show label \"\" a1b1 with greetings\n",
      "\n",
      "### verifying challenge No. 001\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 002\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 003\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 004\n",
      "Compilation Failed!\n",
      "Error: Found identifier but expected 'upload', 'table', tile-type or 'chart'. (Line: 4, Start: 5, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found identifier but expected 'upload', 'table', tile-type or 'chart'. (Line: 4, Start: 5, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found identifier but expected 'upload', 'table', tile-type or 'chart'. (Line: 4, Start: 5, Length: 1, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "a = 13\n",
      "b = 7\n",
      "y = \"y = \\{a} * x + \\{b}\"\n",
      "show text \"\" a1b1 with y\n",
      "\n",
      "### verifying challenge No. 006\n",
      "# compile ok\n",
      "# judge_decision: False\n",
      "# badcode:\n",
      " table T = with\n",
      "  [| as Name, as Score |]\n",
      "  [| \"Alice\", 85 |]\n",
      "  [| \"Bob\", 92 |]\n",
      "  [| \"Charlie\", 88 |]\n",
      "  [| \"David\", 90 |]\n",
      "  [| \"Eve\", 87 |]\n",
      "\n",
      "maxScore = max(T.Score)\n",
      "bestName = argmax(T.Name, T.Score)\n",
      "\n",
      "show scalar \"Max Score\" a1b2 with maxScore\n",
      "show scalar \"Best Name\" c1d2 with bestName\n",
      "# judge explanation:\n",
      " - The student defined a table T with 5 names and scores, which is acceptable.\n",
      "- The student correctly calculated the maximum score, which is acceptable.\n",
      "- The student used argmax function with the wrong arguments. The first argument should be the value to compare (scores) and the second argument should be the index that we want to know (names). This is unacceptable.\n",
      "- The student used \"show scalar\" instead of \"show label\" to display the results. This is acceptable as long as the content is correct.\n",
      "- The student used different variable names (Name, Score) compared to the professor's answer (name, score), which is acceptable.\n",
      "\n",
      "Therefore, the STUDENT ANSWER is NOT_OK.\n",
      "\n",
      "0\n",
      "\n",
      "### verifying challenge No. 007\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Students = with\n",
      "  [| as Name, as Teacher, as Score |]\n",
      "  [| \"Alice\", \"John Doe\", 85 |]\n",
      "  [| \"Bob\", \"Jane Smith\", 70 |]\n",
      "  [| \"Charlie\", \"John Doe\", 92 |]\n",
      "  [| \"David\", \"John Doe\", 78 |]\n",
      "  [| \"Eve\", \"Jane Smith\", 88 |]\n",
      "\n",
      "Students.Successful = Students.Score > 79\n",
      "\n",
      "show table \"Successful Students of John Doe\" a1d5 with\n",
      "  Students[Teacher == \"John Doe\" and Successful]\n",
      "  mean(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Mean Score\"\n",
      "  max(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Best Score\"\n",
      "  argmax(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Best Student\"\n",
      "\n",
      "### verifying challenge No. 008\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "correct:4 out of 8, 50.0%\n"
     ]
    }
   ],
   "source": [
    "# test the pipeline. typically fails on challenges 0,4,7 \n",
    "# (these questions include grammar rules not covered in the documentation)\n",
    "\n",
    "# do not use 005(which is not a question of code generation, does not fit in the pipeline format.)\n",
    "\n",
    "indexes=[\"000\",\"001\",\"002\",\"003\",\"004\",\"006\",\"007\",\"008\"]\n",
    "pipeline_score_allchallenge(indexes,coder_personality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcher-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
