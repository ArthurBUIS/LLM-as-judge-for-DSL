{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from apikey import api_key\n",
    "from myTools import *\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a text literal called greetings with value \"Hello\" and display greetings on the dashboard as a label.   ```envision\n",
      "greeting = \"Hello\" // define the text literal\n",
      "show label greeting // show the text literal as a label. There should be no 'with' !\n",
      "``` Every script must display at least _one_ dashboard _tile_. The following script illustrates how to generate a dashboard that contains a single tile displaying _Hello World!_.\n",
      "\n",
      "```envision\n",
      "greeting = \"Hello World!\"\n",
      "show label greeting\n",
      "```\n",
      "\n",
      "All the statements that start with the keyword `show` indicate that a _tile_ will be displayed. Tiles are the display mechanism provided by Envision. All the tiles present in a script are consolidated in a _dashboard_.\n",
      "\n",
      "The value `\"Hello World!\"` is called a **text literal**, a value that appears verbatim in the code. The operator `=` is the **assignment operator**.\n"
     ]
    }
   ],
   "source": [
    "docu =read_file(\"envision-brief.md\")    \n",
    "challenge = read_file(\"mychallenges/c\"+\"000\"+\".md\")\n",
    "\n",
    "# decompose challenge into question and prof_answer\n",
    "\n",
    "question,prof_answer,references=decompose_challenge(challenge)\n",
    "ref_str=create_ref(references)\n",
    "print(question,prof_answer,ref_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```envision\n",
      "// Define a text literal called greetings with value \"Hello\"\n",
      "greetings = \"Hello\"\n",
      "\n",
      "// Display greetings on the dashboard as a label\n",
      "show label \"Greetings\" a1b1 with greetings\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "coder_personality=\"You are a proficient coder in the Domain Specific Language called Envision. \\\n",
    "    Your task is to generate response to the given challenge. \\\n",
    "    Some challenges will ask you to generate Envision code,\\\n",
    "    others will ask you to explain given code or answer questions related to the Envision language. \\\n",
    "    Do not output any intermediate thinking or explanation, only give the final answer.\\\n",
    "    Here is the documentation of Envision:\\\n",
    "    ### Documentation\\n\" + docu\n",
    "coder_prompt=question\n",
    "\n",
    "coder_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": coder_personality},\n",
    "        {\"role\": \"user\", \"content\": coder_prompt}\n",
    "    ],\n",
    "    max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.4,\n",
    ")\n",
    "stud_sentence=coder_response.choices[0].message.content\n",
    "print(stud_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### QUESTION: define a text literal called greetings with value \"Hello\" and display greetings on the dashboard as a label.  \n",
      "### PROFESSOR ANSWER: ```envision\n",
      "greeting = \"Hello\" // define the text literal\n",
      "show label greeting // show the text literal as a label. There should be no 'with' !\n",
      "```\n",
      "### STUDENT ANSWER: ```envision\n",
      "// Define a text literal called greetings with value \"Hello\"\n",
      "greetings = \"Hello\"\n",
      "\n",
      "// Display greetings on the dashboard as a label\n",
      "show label \"Greetings\" a1b1 with greetings\n",
      "```\n",
      "- The student correctly defines a text literal called `greetings` with the value \"Hello\".\n",
      "- The student correctly displays `greetings` on the dashboard as a label.\n",
      "- The student uses a print position label `a1b1` which is acceptable.\n",
      "- The student uses the label \"Greetings\" instead of \"greeting\" which is acceptable as long as the content is displayed correctly.\n",
      "\n",
      "Therefore, the student answer is **ACCEPTABLE**.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this personality sticks more to professor's answer.\n",
    "\n",
    "judge_personality_teacherAuthority=\"Your goal is to judge the correctness of STUDENT ANSWER, as an answer to the QUESTION.\\\n",
    "In order to judge the STUDENT ANSWER, you are given the PROFESSOR ANSWER with a piece of related documentation.\\\n",
    "Your main job is not to check the syntax correctness, but the logical correctness.\\\n",
    "If the STUDENT ANSWER does not treat the QUESTION logically, it is UNACCEPTABLE.\\\n",
    "Pay special attention to the comments in the PROFESSOR ANSWER. If these comments include\\\n",
    "a rule and if the STUDENT ANSWER violates it, this is UNACCEPTABLE.\\\n",
    "Adding a print position label like a1b2 in the show command is ACCEPTABLE, as long as this is not forbidden by\\\n",
    "the QUESTION or the comments in the PROFESSOR ANSWER.\\\n",
    "The use of extra variable or table to temporarily contain a intermediate quantity is ACCEPTABLE.\\\n",
    "Differences in variable names, column names, table names and label names etc. shall systematically be ACCEPTABLE! \\\n",
    "There are sometimes various ways or logics to treat the same QUESTION, and this is ACCEPTABLE, as long as the goal of the QUESTION is achieved.\\\n",
    "Let's think aloud step by step before making your judgement. Tell each ACCEPTABLE or UNACCEPTABLE point. \\\n",
    "At the end of your output, you should judge 0 if there is anything UNACCEPTABLE (even only 1 mark of UNACCEPTABLE) in the STUDENT ANSWER;\\\n",
    "and judge 1 if everything is ACCEPTABLE.\\\n",
    "you must add a new line which ONLY contains either 0 or 1, strictly according to your judgment.\\\n",
    "This is the piece of related documentation : \\n ## DOCUMENTATION\\n\" + ref_str\n",
    "\n",
    "judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "\n",
    "# Generate a response from the chatbot\n",
    "judge_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": judge_personality_teacherAuthority},\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ],\n",
    "    max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.1,\n",
    ")\n",
    "print(judge_prompt)\n",
    "# Print the generated response\n",
    "print(judge_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Define a text literal called greetings with value \"Hello\"\n",
      "greetings = \"Hello\"\n",
      "\n",
      "// Display greetings on the dashboard as a label\n",
      "show label \"Greetings\" a1b1 with greetings\n"
     ]
    }
   ],
   "source": [
    "# extract the 'real' code from the student answer (cut away the '''envision bit at the start and end)\n",
    "def extract_code(stud_sentence):\n",
    "    lines = stud_sentence.strip().split('\\n')\n",
    "    return '\\n'.join(lines[1:-1])\n",
    "print(extract_code(stud_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 5, Start: 29, Length: 4, Severity: Error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "# send code to online compiler and check if it compiles\n",
    "\n",
    "def check_compilation(script):\n",
    "    url = \"https://try.lokad.com/w/script/trycompile\"\n",
    "    payload = {\n",
    "        \"Script\": script\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send POST request\n",
    "        response = requests.post(url, json=payload)\n",
    "\n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if result[\"IsCompOk\"]:\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Compilation Failed!\")\n",
    "                for message in result[\"CompMessages\"]:\n",
    "                    print(f\"Error: {message['Text']} (Line: {message['Line']}, Start: {message['Start']}, Length: {message['Length']}, Severity: {message['Severity']})\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(\"Error: Unable to reach the compilation service.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "check_compilation(extract_code(stud_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each question, try 3 generation-compilations.\n",
    "# if compiles, further check with judge.\n",
    "def pipeline_verify(challenge,coder_personality,judge_personality=judge_personality_teacherAuthority):\n",
    "    question,prof_answer=decompose_challenge(challenge)\n",
    "    n_tries=3\n",
    "\n",
    "    for compile_try in range(n_tries):\n",
    "        coder_prompt=question\n",
    "        coder_response = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": coder_personality},\n",
    "                {\"role\": \"user\", \"content\": coder_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        stud_sentence=coder_response.choices[0].message.content\n",
    "        if (question.split(\"\\n\")[0] ==\\\n",
    "        '# this question expects a textual answer and not generation of code. #'):\n",
    "            print('# theoretical question, no compile.')\n",
    "            break\n",
    "        if(check_compilation(extract_code(stud_sentence))):\n",
    "            print('# compile ok')\n",
    "            break\n",
    "        elif (compile_try==n_tries-1):\n",
    "            print( \"# too many failures !\")\n",
    "            print('# badcode:\\n'+extract_code(stud_sentence))\n",
    "            return stud_sentence,\"too many failures !\",False\n",
    "\n",
    "    judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "    judge_response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge_personality},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ],\n",
    "        max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    judge_sentence=judge_response.choices[0].message.content\n",
    "    judge_decision=judge_sentence[-1]=='1'\n",
    "    print ('# judge_decision:',judge_decision)\n",
    "    if not judge_decision:\n",
    "        print('# badcode:\\n',extract_code(stud_sentence))\n",
    "        print('# judge explanation:\\n',judge_sentence)\n",
    "    return stud_sentence,judge_sentence,judge_decision\n",
    "# a all-in-one function to score a model on a list of challenges\n",
    "def pipeline_score_allchallenge(indexes,coder_personality):\n",
    "    challenges=[readfile(\"mychallenges/c\"+index+\".md\") for index in indexes]\n",
    "    score=0\n",
    "    for i in range(len(challenges)):\n",
    "        challenge=challenges[i]\n",
    "        print('\\n### verifying challenge No. '+indexes[i])\n",
    "        _,_,judge_decision=pipeline_verify(challenge,coder_personality)\n",
    "        if (judge_decision): score+=1\n",
    "    print('correct:'+str(score)+' out of '+str(len(challenges))+', '+str(score/len(challenges)*100)+'%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### verifying challenge No. 000\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 2, Start: 20, Length: 4, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 2, Start: 20, Length: 4, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: 'show %s' expects no 'with'. (Line: 2, Start: 20, Length: 4, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "greetings = \"Hello\"\n",
      "show label \"\" a1b1 with greetings\n",
      "\n",
      "### verifying challenge No. 001\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 002\n",
      "Compilation Failed!\n",
      "Error: Cannot broadcast T data to Scalar data. (Line: 9, Start: 27, Length: 3, Severity: Error)\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 003\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. 004\n",
      "Compilation Failed!\n",
      "Error: Undefined variable 'Scalar.x'. (Line: 1, Start: 10, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Invalid token (Line: 1, Start: 12, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Invalid token (Line: 1, Start: 12, Length: 1, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "m = \"y = \\{13} * x + \\{7}\"\n",
      "show text \"\" a1b1 with m\n",
      "\n",
      "### verifying challenge No. 005\n",
      "# theoretical question, no compile.\n",
      "# judge_decision: False\n",
      "# badcode:\n",
      " \n",
      "- A: 4\n",
      "- B: 3\n",
      "- C: 6\n",
      "\n",
      "The script filters the table T based on the values in the X column. \n",
      "- A is the sum of X values where X is not equal to 2, which is 1 + 3 = 4.\n",
      "- B is the sum of X values where X is not equal to 1, which is 2 + 3 = 5.\n",
      "# judge explanation:\n",
      " The STUDENT ANSWER is NOT_OK.\n",
      "\n",
      "1. The value of B is incorrectly computed as 5 instead of 3. The student incorrectly calculated the sum of X values where X is not equal to 1 as 2 + 3 = 5, instead of correctly identifying that only the value 3 remains after filtering out 1.\n",
      "\n",
      "Therefore, the overall answer is NOT_OK. \n",
      "\n",
      "0\n",
      "\n",
      "### verifying challenge No. 006\n",
      "# compile ok\n",
      "# judge_decision: False\n",
      "# badcode:\n",
      " table T = with\n",
      "  [| as Name, as Score |]\n",
      "  [| \"Alice\", 85 |]\n",
      "  [| \"Bob\", 92 |]\n",
      "  [| \"Charlie\", 88 |]\n",
      "  [| \"David\", 90 |]\n",
      "  [| \"Eve\", 87 |]\n",
      "\n",
      "maxScore = max(T.Score)\n",
      "bestName = argmax(T.Name, T.Score)\n",
      "\n",
      "show scalar \"\" a1b2 with maxScore\n",
      "show scalar \"\" c1d2 with bestName\n",
      "# judge explanation:\n",
      " - The student's table creation is OK, as it defines a table T with 5 names and corresponding scores.\n",
      "- The calculation of the maximum score is OK.\n",
      "- The calculation of the best name is NOT_OK. The argmax function should take the score column as the first argument and the name column as the second argument.\n",
      "- The display of the maximum score at tile a1b2 is OK.\n",
      "- The display of the best student at c1d2 is NOT_OK. The best student should be displayed based on the name that achieved the best score, not the name column itself.\n",
      "\n",
      "Overall, the STUDENT ANSWER is NOT_OK.\n",
      "\n",
      "0\n",
      "\n",
      "### verifying challenge No. 007\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Students = with\n",
      "  [| as Name, as Teacher, as Score |]\n",
      "  [| \"Alice\", \"John Doe\", 85 |]\n",
      "  [| \"Bob\", \"Jane Smith\", 70 |]\n",
      "  [| \"Charlie\", \"John Doe\", 92 |]\n",
      "  [| \"David\", \"John Doe\", 78 |]\n",
      "  [| \"Eve\", \"John Doe\", 88 |]\n",
      "\n",
      "Students.Successful = Students.Score > 79\n",
      "\n",
      "show table \"Successful Students of John Doe\" a1d6 with\n",
      "  Students[Teacher == \"John Doe\" and Successful]\n",
      "  mean(Students.Score) as \"Mean Score\"\n",
      "  max(Students.Score) as \"Best Score\"\n",
      "  argmax(Students.Score) as \"Best Student\"\n",
      "\n",
      "### verifying challenge No. 008\n",
      "Compilation Failed!\n",
      "Error: Found indent but expected 'show', 'with', 'for', 'each', '_', identifier, 'write', 'schema', 'montecarlo', 'autodiff', '@', 'def', 'delete', 'expect', '[|', 'read', '{', 'table', 'loop', 'import', 'return', 'dash', 'sample', 'const', 'export', 'if', 'span', 'where', 'keep' or end-of-script. (Line: 8, Start: 1, Length: 2, Severity: Error)\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "correct:4 out of 9, 44.44444444444444%\n"
     ]
    }
   ],
   "source": [
    "# test the pipeline. typically fails on challenges 0,4,7 \n",
    "# (these questions include grammar rules not covered in the documentation)\n",
    "\n",
    "indexes=[\"000\",\"001\",\"002\",\"003\",\"004\",\"005\",\"006\",\"007\",\"008\"]\n",
    "pipeline_score_allchallenge(indexes,coder_personality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcher-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
